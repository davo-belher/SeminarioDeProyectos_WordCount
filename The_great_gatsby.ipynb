{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e38da145",
      "metadata": {
        "id": "e38da145"
      },
      "source": [
        "## html_scraping_bookwordcount\n",
        "\n",
        "Script to extract words from a html book, find the most commonly occuring words and save them to a csv file.\n",
        "\n",
        "Note: \"stop words\" are excluded, currently set to English.\n",
        "\n",
        "Recommended source of books is from the Guttenburg Project, where the html versions should be chosen.\n",
        "<br>Most popular books: https://gutenberg.org/browse/scores/top\n",
        "<br>Books in Spanish: https://gutenberg.org/browse/languages/es\n",
        "\n",
        "Code modified from DataCamp.com 'Word Frequency' project.\n",
        "\n",
        "Some packages will likely require installing for the script to run.\n",
        "\n",
        "Merlin Fair\n",
        "<br>Created: 2024/02/27\n",
        "<br>Last Modified: 2024/02/27 (MF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c02a9dbf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c02a9dbf",
        "outputId": "2d8cd0e2-9b38-413a-d40f-feb6beba310e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Import and download packages\n",
        "import requests\n",
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from collections import Counter\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "975173a4",
      "metadata": {
        "id": "975173a4"
      },
      "outputs": [],
      "source": [
        "# Get the Moby Dick HTML\n",
        "r = requests.get('https://gutenberg.org/cache/epub/64317/pg64317-images.html')\n",
        "\n",
        "# Set the correct text encoding of the HTML page\n",
        "r.encoding = 'utf-8'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e712c0a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e712c0a7",
        "outputId": "6c495d38-b281-4a15-f39c-80d06a867317"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ight was a colossal affair by any standard—it was a factual imitation of some Hôtel de Ville in Normandy, with a tower on one side, spanking new under a thin beard of raw ivy, and a marble swimming pool, and more than forty acres of lawn and garden. It was Gatsby’s mansion. Or, rather, as I didn’t know Mr. Gatsby, it was a mansion inhabited by a gentleman of that name. My own house was an eyesore, but it was a small eyesore, and it had been overlooked, so I had a view of the water, a partial view of my neighbour’s lawn, and the consoling proximity of millionaires—all for eighty dollars a month.\r\n",
            "</p>\r\n",
            "<p>\r\n",
            "Across the courtesy bay the white palaces of fashionable East Egg glittered along the water, and the history of the summer really begins on the evening I drove over there to have dinner with the Tom Buchanans. Daisy was my second cousin once removed, and I’d known Tom in college. And just after the war I spent two days with them in Chicago.\r\n",
            "</p>\r\n",
            "<p>\r\n",
            "Her husband, among various phys\n"
          ]
        }
      ],
      "source": [
        "# Extract the HTML from the request object\n",
        "html = r.text\n",
        "\n",
        "# Print 1000 characters in html\n",
        "print(html[15000:16000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f6ff9b93",
      "metadata": {
        "id": "f6ff9b93"
      },
      "outputs": [],
      "source": [
        "# Create a BeautifulSoup object from the HTML\n",
        "html_soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "# Get the text out of the soup\n",
        "moby_text = html_soup.get_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "71736727",
      "metadata": {
        "id": "71736727"
      },
      "outputs": [],
      "source": [
        "# Create a tokenizer\n",
        "tokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = tokenizer.tokenize(moby_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ea3ffd9e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea3ffd9e",
        "outputId": "5175014e-94a2-495c-ae42-015776932e89"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'great', 'gatsby']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Create a list called words containing all tokens transformed to lowercase\n",
        "words = [token.lower() for token in tokens]\n",
        "\n",
        "# Print out the first eight words\n",
        "words[:8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c6ce646f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6ce646f",
        "outputId": "7a30c6f9-dd30-47b1-878b-5de5a776389a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Get the English stop words from nltk\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "# Print out the first eight stop words\n",
        "stop_words[:8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1984ee32",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1984ee32",
        "outputId": "3dcbc1bb-7a91-4cb9-d909-99a626906064"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['project', 'gutenberg', 'ebook', 'great', 'gatsby']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Create a list words_ns containing all words that are in words but not in stop_words\n",
        "words_no_stop = [word for word in words if word not in stop_words]\n",
        "\n",
        "# Print the first five words_no_stop to check that stop words are gone\n",
        "words_no_stop[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "4dd018a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dd018a6",
        "outputId": "ff7ad0a0-3703-4b33-8fa0-de8bc47d6cad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('gatsby', 268), ('said', 235), ('tom', 191), ('daisy', 186), ('one', 154), ('like', 122), ('man', 114), ('back', 109), ('came', 108), ('little', 103)]\n"
          ]
        }
      ],
      "source": [
        "# Initialize a Counter object from our processed list of words\n",
        "count_total = Counter(words_no_stop)\n",
        "\n",
        "# Store ten most common words and their counts as top_ten\n",
        "top_ten = count_total.most_common(10)\n",
        "\n",
        "# Print the top ten words and their counts\n",
        "print(top_ten)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "0b0eab20",
      "metadata": {
        "id": "0b0eab20"
      },
      "outputs": [],
      "source": [
        "# Saving top 100 to CSV file\n",
        "# N.b. change filename to reflect chosen book\n",
        "filename = 'words_mobydick.csv'\n",
        "with open(filename, 'w', newline='') as csvfile:\n",
        "\n",
        "    writer = csv.writer(csvfile, delimiter=',',  quotechar='\"',\n",
        "                                     quoting=csv.QUOTE_MINIMAL)\n",
        "    writer.writerow([\"word\",\"count\"])\n",
        "    for key, count in count_total.most_common(100):\n",
        "        word = key\n",
        "        writer.writerow([word, count])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}